#!/usr/bin/env python3
"""
BFCL í‰ê°€ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # í€µ í…ŒìŠ¤íŠ¸ (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)
    python run_eval.py --quick
    
    # íŠ¹ì • ëª¨ë¸ë§Œ í€µ í…ŒìŠ¤íŠ¸
    python run_eval.py --quick --models openrouter/qwen3-14b-FC
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸
    python run_eval.py --full
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ
    python run_eval.py --quick --categories simple_python,multiple

ê²°ê³¼ë¬¼:
    reports/
    â”œâ”€â”€ {model_name}/
    â”‚   â””â”€â”€ {model_name}_eval_report.xlsx
    â””â”€â”€ summary/
        â””â”€â”€ all_models_summary.xlsx
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# ì§€ì› ëª¨ë¸ ëª©ë¡
SUPPORTED_MODELS = [
    "openrouter/llama-3.3-70b-instruct-FC",
    "openrouter/mistral-small-3.2-24b-instruct-FC",
    "openrouter/qwen3-32b-FC",
    "openrouter/qwen3-14b-FC",
    "openrouter/qwen3-next-80b-a3b-instruct-FC",
]

# í€µ í…ŒìŠ¤íŠ¸ìš© ID (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)
QUICK_TEST_IDS = {
    "simple_python": ["simple_python_0", "simple_python_1"],
    "multiple": ["multiple_0", "multiple_1"],
    "parallel": ["parallel_0", "parallel_1"],
}

# ì „ì²´ ì¹´í…Œê³ ë¦¬
ALL_CATEGORIES = ["simple_python", "multiple", "parallel"]

def _write_sample_id_file(categories: list[str], sample_size: int, seed: int = 0) -> str:
    """
    bfcl_eval generateì˜ --run-ids ëª¨ë“œìš© íŒŒì¼ì„ ì„ì‹œë¡œ ì‘ì„±.
    - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë™ì¼í•œ ID ì§‘í•©ì„ ëª¨ë“  ëª¨ë¸ì— ì ìš© (ëª¨ë¸ ë¹„êµ ê°€ëŠ¥)
    - ê¸°ì¡´ íŒŒì¼ì€ .bak ìœ¼ë¡œ ë°±ì—… í›„, ì‘ì—… ì¢…ë£Œ ì‹œ ë³µêµ¬ í•„ìš”
    """
    from bfcl_eval.utils import load_dataset_entry
    import random

    rng = random.Random(seed)
    sample_map: dict[str, list[str]] = {}

    for cat in categories:
        entries = load_dataset_entry(cat)
        ids = [e["id"] for e in entries if "id" in e]
        # ì•ˆì •ì ì¸ ìˆœì„œë¥¼ ìœ„í•´ ì •ë ¬ í›„ ìƒ˜í”Œë§
        ids = sorted(ids)
        if sample_size >= len(ids):
            picked = ids
        else:
            # ì¬í˜„ ê°€ëŠ¥í•˜ê²Œ ëœë¤ ìƒ˜í”Œ
            picked = rng.sample(ids, sample_size)
            picked = sorted(picked)
        sample_map[cat] = picked

    test_ids_file = Path("test_case_ids_to_generate.json")
    bak_path = test_ids_file.with_suffix(".json.bak")
    if test_ids_file.exists():
        bak_path.write_text(test_ids_file.read_text())
    test_ids_file.write_text(json.dumps(sample_map, indent=2))
    return str(bak_path)


def _restore_id_file(bak_path: str) -> None:
    test_ids_file = Path("test_case_ids_to_generate.json")
    bak = Path(bak_path)
    if bak.exists():
        test_ids_file.write_text(bak.read_text())
        bak.unlink()


def run_command(cmd: list, description: str = "") -> bool:
    """ëª…ë ¹ ì‹¤í–‰"""
    if description:
        print(f"\n{'='*60}")
        print(f"ğŸ“Œ {description}")
        print(f"{'='*60}")
    
    print(f"$ {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=False)
    return result.returncode == 0


def clean_directories():
    """ì´ì „ ê²°ê³¼ ì •ë¦¬"""
    print("\nğŸ§¹ ì´ì „ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    for dir_name in ["result", "score"]:
        dir_path = Path(dir_name)
        if dir_path.exists():
            shutil.rmtree(dir_path)
        dir_path.mkdir(parents=True, exist_ok=True)
    print("âœ… ì •ë¦¬ ì™„ë£Œ")


def setup_quick_test():
    """í€µ í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
    test_ids_file = Path("test_case_ids_to_generate.json")
    with open(test_ids_file, "w") as f:
        json.dump(QUICK_TEST_IDS, f, indent=2)
    print(f"âœ… í€µ í…ŒìŠ¤íŠ¸ ì„¤ì • ì™„ë£Œ: {test_ids_file}")


def generate_results(model: str, categories: list, is_quick: bool) -> bool:
    """ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    cmd = [
        sys.executable, "-m", "bfcl_eval", "generate",
        "--model", model,
        "--test-category", ",".join(categories),
        "--temperature", "0",
        "--num-threads", "1",
    ]
    
    if is_quick:
        cmd.append("--run-ids")
    
    return run_command(cmd, f"ì‘ë‹µ ìƒì„±: {model}")


def evaluate_results(models: list, categories: list) -> bool:
    """í‰ê°€ ì‹¤í–‰"""
    cmd = [
        sys.executable, "-m", "bfcl_eval", "evaluate",
        "--model", ",".join(models),
        "--test-category", ",".join(categories),
        "--partial-eval",
    ]
    
    return run_command(cmd, "í‰ê°€ ì‹¤í–‰")


def generate_reports(models: list):
    """ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±"""
    from excel_reporter import generate_excel_report, generate_all_models_summary
    
    reports_dir = Path("reports")
    reports_dir.mkdir(exist_ok=True)
    
    print(f"\n{'='*60}")
    print("ğŸ“Š ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±")
    print(f"{'='*60}")
    
    # ëª¨ë¸ë³„ ë³´ê³ ì„œ ìƒì„±
    model_reports = []
    
    for model in models:
        safe_name = model.replace("/", "_")
        result_dir = Path(f"result/{safe_name}")
        score_dir = Path(f"score/{safe_name}")
        
        if not result_dir.exists():
            print(f"âš ï¸  ê²°ê³¼ ì—†ìŒ: {model}")
            continue
        
        # ëª¨ë¸ë³„ í´ë” ìƒì„±
        model_report_dir = reports_dir / safe_name
        model_report_dir.mkdir(exist_ok=True)
        
        # ë³´ê³ ì„œ ìƒì„±
        try:
            report_path = generate_excel_report(
                model_name=model,
                result_dir=str(result_dir),
                score_dir=str(score_dir)
            )
            
            # ìƒì„±ëœ íŒŒì¼ì„ ëª¨ë¸ í´ë”ë¡œ ì´ë™
            report_file = Path(report_path)
            target_path = model_report_dir / f"{safe_name}_eval_report.xlsx"
            if report_file.exists():
                shutil.move(str(report_file), str(target_path))
                print(f"âœ… {model}: {target_path}")
                model_reports.append(target_path)
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨ ({model}): {e}")
    
    # ì „ì²´ ì·¨í•© ë³´ê³ ì„œ ìƒì„±
    if len(model_reports) > 1:
        try:
            summary_path = generate_all_models_summary(str(reports_dir))
            print(f"âœ… ì „ì²´ ì·¨í•©: {summary_path}")
        except Exception as e:
            print(f"âš ï¸  ì·¨í•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    return model_reports


def print_summary(models: list):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print("ğŸ“‹ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    # score íŒŒì¼ì—ì„œ ê²°ê³¼ ì½ê¸°
    for model in models:
        safe_name = model.replace("/", "_")
        score_dir = Path(f"score/{safe_name}/non_live")
        
        if not score_dir.exists():
            continue
        
        print(f"\nğŸ¦ {model}")
        
        for score_file in sorted(score_dir.glob("*_score.json")):
            with open(score_file) as f:
                first_line = f.readline()
                summary = json.loads(first_line)
            
            category = score_file.name.replace("BFCL_v4_", "").replace("_score.json", "")
            accuracy = summary.get("accuracy", 0) * 100
            correct = summary.get("correct_count", 0)
            total = summary.get("total_count", 0)
            
            status = "âœ…" if accuracy >= 80 else "âš ï¸" if accuracy >= 50 else "âŒ"
            print(f"   {status} {category}: {accuracy:.1f}% ({correct}/{total})")


def main():
    parser = argparse.ArgumentParser(description="BFCL í‰ê°€ ìë™í™”")
    parser.add_argument("--quick", action="store_true", help="í€µ í…ŒìŠ¤íŠ¸ (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)")
    parser.add_argument("--full", action="store_true", help="ì „ì²´ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--models", type=str, help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ì‰¼í‘œ êµ¬ë¶„)")
    parser.add_argument("--categories", type=str, help="í…ŒìŠ¤íŠ¸í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œ êµ¬ë¶„)")
    parser.add_argument("--sample-size", type=int, default=0, help="ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ê°œìˆ˜(0ì´ë©´ ì „ì²´). --run-idsë¡œ ì‹¤í–‰")
    parser.add_argument("--sample-seed", type=int, default=0, help="ìƒ˜í”Œë§ ì‹œë“œ(ì¬í˜„ì„±)")
    parser.add_argument("--num-threads", type=int, default=1, help="ìƒì„± ë‹¨ê³„ ë™ì‹œì„±(threads)")
    parser.add_argument("--append", action="store_true", help="ê¸°ì¡´ result/scoreë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³  ëˆ„ì  ì‹¤í–‰")
    parser.add_argument("--skip-generate", action="store_true", help="ìƒì„± ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-evaluate", action="store_true", help="í‰ê°€ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--report-only", action="store_true", help="ë³´ê³ ì„œë§Œ ìƒì„±")
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’:
    # - ì¹´í…Œê³ ë¦¬ ì§€ì •ì´ ì—†ìœ¼ë©´ quick
    # - ì¹´í…Œê³ ë¦¬ë¥¼ ì§€ì •í•˜ë©´ ì „ì²´(ë˜ëŠ” sample-size) ì‹¤í–‰
    if not args.quick and not args.full:
        args.quick = (args.categories is None and args.sample_size == 0)
    
    # ëª¨ë¸ ëª©ë¡
    if args.models:
        models = [m.strip() for m in args.models.split(",")]
    else:
        models = SUPPORTED_MODELS
    
    # ì¹´í…Œê³ ë¦¬ ëª©ë¡
    if args.categories:
        categories = [c.strip() for c in args.categories.split(",")]
    else:
        categories = ALL_CATEGORIES
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    BFCL í‰ê°€ ìë™í™”                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ëª¨ë“œ: {'í€µ í…ŒìŠ¤íŠ¸' if args.quick else 'ì „ì²´ í…ŒìŠ¤íŠ¸'}                                          â•‘
â•‘  ëª¨ë¸: {len(models)}ê°œ                                                â•‘
â•‘  ì¹´í…Œê³ ë¦¬: {', '.join(categories):<43} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # 1. ì •ë¦¬
    if not args.report_only and not args.skip_generate and not args.append:
        clean_directories()
        
        if args.quick:
            setup_quick_test()
    
    # 2. ìƒì„±
    if not args.report_only and not args.skip_generate:
        # sample-size ëª¨ë“œë©´, run-ids íŒŒì¼ì„ ì„ì‹œë¡œ êµì²´í•´ì„œ í•´ë‹¹ IDë§Œ ìƒì„±
        bak_path = ""
        try:
            if args.sample_size and args.sample_size > 0:
                bak_path = _write_sample_id_file(categories, args.sample_size, seed=args.sample_seed)
                is_run_ids = True
            else:
                is_run_ids = args.quick

            # num-threads ë°˜ì˜
            def _generate(model: str) -> bool:
                cmd = [
                    sys.executable, "-m", "bfcl_eval", "generate",
                    "--model", model,
                    "--test-category", ",".join(categories),
                    "--temperature", "0",
                    "--num-threads", str(args.num_threads),
                ]
                if is_run_ids:
                    cmd.append("--run-ids")
                return run_command(cmd, f"ì‘ë‹µ ìƒì„±: {model}")

            for model in models:
                if not _generate(model):
                    print(f"âŒ ìƒì„± ì‹¤íŒ¨: {model}")
        finally:
            if bak_path:
                _restore_id_file(bak_path)
    
    # 3. í‰ê°€
    if not args.report_only and not args.skip_evaluate:
        if not evaluate_results(models, categories):
            print("âŒ í‰ê°€ ì‹¤íŒ¨")
    
    # 4. ë³´ê³ ì„œ ìƒì„±
    generate_reports(models)
    
    # 5. ìš”ì•½ ì¶œë ¥
    print_summary(models)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ğŸ“ ë³´ê³ ì„œ ìœ„ì¹˜: reports/")


if __name__ == "__main__":
    main()
