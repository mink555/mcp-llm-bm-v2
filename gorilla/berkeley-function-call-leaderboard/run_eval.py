#!/usr/bin/env python3
"""
BFCL í‰ê°€ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # í€µ í…ŒìŠ¤íŠ¸ (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)
    python run_eval.py --quick
    
    # íŠ¹ì • ëª¨ë¸ë§Œ í€µ í…ŒìŠ¤íŠ¸
    python run_eval.py --quick --models openrouter/qwen3-14b-FC
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸
    python run_eval.py --full
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ
    python run_eval.py --quick --categories simple_python,multiple

ê²°ê³¼ë¬¼:
    reports/
    â”œâ”€â”€ {model_name}/
    â”‚   â””â”€â”€ {model_name}_eval_report.xlsx
    â””â”€â”€ summary/
        â””â”€â”€ all_models_summary.xlsx
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# ì§€ì› ëª¨ë¸ ëª©ë¡
SUPPORTED_MODELS = [
    "openrouter/llama-3.3-70b-instruct-FC",
    "openrouter/mistral-small-3.2-24b-instruct-FC",
    "openrouter/qwen3-32b-FC",
    "openrouter/qwen3-14b-FC",
    "openrouter/qwen3-next-80b-a3b-instruct-FC",
]

# í€µ í…ŒìŠ¤íŠ¸ìš© ID (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)
QUICK_TEST_IDS = {
    "simple_python": ["simple_python_0", "simple_python_1"],
    "multiple": ["multiple_0", "multiple_1"],
    "parallel": ["parallel_0", "parallel_1"],
}

# ì „ì²´ ì¹´í…Œê³ ë¦¬
ALL_CATEGORIES = ["simple_python", "multiple", "parallel"]


def run_command(cmd: list, description: str = "") -> bool:
    """ëª…ë ¹ ì‹¤í–‰"""
    if description:
        print(f"\n{'='*60}")
        print(f"ğŸ“Œ {description}")
        print(f"{'='*60}")
    
    print(f"$ {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=False)
    return result.returncode == 0


def clean_directories():
    """ì´ì „ ê²°ê³¼ ì •ë¦¬"""
    print("\nğŸ§¹ ì´ì „ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    for dir_name in ["result", "score"]:
        dir_path = Path(dir_name)
        if dir_path.exists():
            shutil.rmtree(dir_path)
        dir_path.mkdir(parents=True, exist_ok=True)
    print("âœ… ì •ë¦¬ ì™„ë£Œ")


def setup_quick_test():
    """í€µ í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
    test_ids_file = Path("test_case_ids_to_generate.json")
    with open(test_ids_file, "w") as f:
        json.dump(QUICK_TEST_IDS, f, indent=2)
    print(f"âœ… í€µ í…ŒìŠ¤íŠ¸ ì„¤ì • ì™„ë£Œ: {test_ids_file}")


def generate_results(model: str, categories: list, is_quick: bool) -> bool:
    """ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    cmd = [
        "python", "-m", "bfcl_eval", "generate",
        "--model", model,
        "--test-category", ",".join(categories),
        "--temperature", "0",
        "--num-threads", "1",
    ]
    
    if is_quick:
        cmd.append("--run-ids")
    
    return run_command(cmd, f"ì‘ë‹µ ìƒì„±: {model}")


def evaluate_results(models: list, categories: list) -> bool:
    """í‰ê°€ ì‹¤í–‰"""
    cmd = [
        "python", "-m", "bfcl_eval", "evaluate",
        "--model", ",".join(models),
        "--test-category", ",".join(categories),
        "--partial-eval",
    ]
    
    return run_command(cmd, "í‰ê°€ ì‹¤í–‰")


def generate_reports(models: list):
    """ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±"""
    from excel_reporter import BFCLExcelReporter
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
    
    reports_dir = Path("reports")
    reports_dir.mkdir(exist_ok=True)
    
    # ìŠ¤íƒ€ì¼ ì •ì˜
    HEADER_FILL = PatternFill(start_color="D9E2EC", end_color="D9E2EC", fill_type="solid")
    HEADER_FONT = Font(bold=True, size=11)
    THIN_BORDER = Border(
        left=Side(style="thin"), right=Side(style="thin"),
        top=Side(style="thin"), bottom=Side(style="thin"),
    )
    PASS_FILL = PatternFill(start_color="D1FAE5", end_color="D1FAE5", fill_type="solid")
    FAIL_FILL = PatternFill(start_color="FEE2E2", end_color="FEE2E2", fill_type="solid")
    
    print(f"\n{'='*60}")
    print("ğŸ“Š ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±")
    print(f"{'='*60}")
    
    # ëª¨ë¸ë³„ ë³´ê³ ì„œ ìƒì„±
    model_reports = []
    all_results = {}  # ì „ì²´ ì·¨í•©ìš©
    
    for model in models:
        safe_name = model.replace("/", "_")
        result_dir = Path(f"result/{safe_name}")
        score_dir = Path(f"score/{safe_name}")
        
        if not result_dir.exists():
            print(f"âš ï¸  ê²°ê³¼ ì—†ìŒ: {model}")
            continue
        
        # ëª¨ë¸ë³„ í´ë” ìƒì„±
        model_report_dir = reports_dir / safe_name
        model_report_dir.mkdir(exist_ok=True)
        
        # ë³´ê³ ì„œ ìƒì„±
        reporter = BFCLExcelReporter(
            model_name=model,
            result_dir=result_dir,
            score_dir=score_dir,
        )
        reporter.load_data()
        reporter.create_evaluation_criteria_sheet()
        reporter.create_detail_sheet()
        reporter.create_summary_sheet()
        
        report_path = model_report_dir / f"{safe_name}_eval_report.xlsx"
        reporter.wb.save(report_path)
        
        print(f"âœ… {model}: {report_path}")
        model_reports.append(report_path)
        
        # ì·¨í•©ìš© ë°ì´í„° ìˆ˜ì§‘
        all_results[model] = {
            "categories": reporter.categories_found,
            "detail_data": reporter.detail_data,
        }
    
    # ì „ì²´ ì·¨í•© ë³´ê³ ì„œ ìƒì„±
    if len(all_results) > 1:
        summary_dir = reports_dir / "summary"
        summary_dir.mkdir(exist_ok=True)
        
        wb = Workbook()
        ws = wb.active
        ws.title = "Model Comparison"
        
        # í—¤ë”
        headers = ["Model", "Category", "Total", "Correct", "Incorrect", "Accuracy"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.fill = HEADER_FILL
            cell.font = HEADER_FONT
            cell.alignment = Alignment(horizontal="center")
            cell.border = THIN_BORDER
        
        row = 2
        for model, data in all_results.items():
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ê³„ì‚°
            category_stats = {}
            for entry in data["detail_data"]:
                cat = entry["category"]
                if cat not in category_stats:
                    category_stats[cat] = {"total": 0, "correct": 0}
                category_stats[cat]["total"] += 1
                if entry["result"] == "PASS":
                    category_stats[cat]["correct"] += 1
            
            for cat, stats in sorted(category_stats.items()):
                total = stats["total"]
                correct = stats["correct"]
                incorrect = total - correct
                accuracy = correct / total if total > 0 else 0
                
                ws.cell(row=row, column=1, value=model).border = THIN_BORDER
                ws.cell(row=row, column=2, value=cat).border = THIN_BORDER
                ws.cell(row=row, column=3, value=total).border = THIN_BORDER
                ws.cell(row=row, column=4, value=correct).border = THIN_BORDER
                ws.cell(row=row, column=5, value=incorrect).border = THIN_BORDER
                
                acc_cell = ws.cell(row=row, column=6, value=accuracy)
                acc_cell.number_format = "0.00%"
                acc_cell.border = THIN_BORDER
                if accuracy >= 0.8:
                    acc_cell.fill = PASS_FILL
                elif accuracy < 0.5:
                    acc_cell.fill = FAIL_FILL
                
                row += 1
        
        # ì—´ ë„ˆë¹„ ì¡°ì •
        ws.column_dimensions["A"].width = 45
        ws.column_dimensions["B"].width = 20
        ws.column_dimensions["C"].width = 10
        ws.column_dimensions["D"].width = 10
        ws.column_dimensions["E"].width = 10
        ws.column_dimensions["F"].width = 12
        
        summary_path = summary_dir / "all_models_summary.xlsx"
        wb.save(summary_path)
        print(f"âœ… ì „ì²´ ì·¨í•©: {summary_path}")
    
    return model_reports


def print_summary(models: list):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print("ğŸ“‹ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    # score íŒŒì¼ì—ì„œ ê²°ê³¼ ì½ê¸°
    for model in models:
        safe_name = model.replace("/", "_")
        score_dir = Path(f"score/{safe_name}/non_live")
        
        if not score_dir.exists():
            continue
        
        print(f"\nğŸ¦ {model}")
        
        for score_file in sorted(score_dir.glob("*_score.json")):
            with open(score_file) as f:
                first_line = f.readline()
                summary = json.loads(first_line)
            
            category = score_file.name.replace("BFCL_v4_", "").replace("_score.json", "")
            accuracy = summary.get("accuracy", 0) * 100
            correct = summary.get("correct_count", 0)
            total = summary.get("total_count", 0)
            
            status = "âœ…" if accuracy >= 80 else "âš ï¸" if accuracy >= 50 else "âŒ"
            print(f"   {status} {category}: {accuracy:.1f}% ({correct}/{total})")


def main():
    parser = argparse.ArgumentParser(description="BFCL í‰ê°€ ìë™í™”")
    parser.add_argument("--quick", action="store_true", help="í€µ í…ŒìŠ¤íŠ¸ (ê° ì¹´í…Œê³ ë¦¬ 2ê°œì”©)")
    parser.add_argument("--full", action="store_true", help="ì „ì²´ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--models", type=str, help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ì‰¼í‘œ êµ¬ë¶„)")
    parser.add_argument("--categories", type=str, help="í…ŒìŠ¤íŠ¸í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œ êµ¬ë¶„)")
    parser.add_argument("--skip-generate", action="store_true", help="ìƒì„± ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-evaluate", action="store_true", help="í‰ê°€ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--report-only", action="store_true", help="ë³´ê³ ì„œë§Œ ìƒì„±")
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’: í€µ í…ŒìŠ¤íŠ¸
    if not args.quick and not args.full:
        args.quick = True
    
    # ëª¨ë¸ ëª©ë¡
    if args.models:
        models = [m.strip() for m in args.models.split(",")]
    else:
        models = SUPPORTED_MODELS
    
    # ì¹´í…Œê³ ë¦¬ ëª©ë¡
    if args.categories:
        categories = [c.strip() for c in args.categories.split(",")]
    else:
        categories = ALL_CATEGORIES
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    BFCL í‰ê°€ ìë™í™”                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ëª¨ë“œ: {'í€µ í…ŒìŠ¤íŠ¸' if args.quick else 'ì „ì²´ í…ŒìŠ¤íŠ¸'}                                          â•‘
â•‘  ëª¨ë¸: {len(models)}ê°œ                                                â•‘
â•‘  ì¹´í…Œê³ ë¦¬: {', '.join(categories):<43} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # 1. ì •ë¦¬
    if not args.report_only and not args.skip_generate:
        clean_directories()
        
        if args.quick:
            setup_quick_test()
    
    # 2. ìƒì„±
    if not args.report_only and not args.skip_generate:
        for model in models:
            if not generate_results(model, categories, args.quick):
                print(f"âŒ ìƒì„± ì‹¤íŒ¨: {model}")
    
    # 3. í‰ê°€
    if not args.report_only and not args.skip_evaluate:
        if not evaluate_results(models, categories):
            print("âŒ í‰ê°€ ì‹¤íŒ¨")
    
    # 4. ë³´ê³ ì„œ ìƒì„±
    generate_reports(models)
    
    # 5. ìš”ì•½ ì¶œë ¥
    print_summary(models)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ğŸ“ ë³´ê³ ì„œ ìœ„ì¹˜: reports/")


if __name__ == "__main__":
    main()
